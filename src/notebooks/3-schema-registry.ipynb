{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Schema Registry in Apache Kafka\n",
                "\n",
                "A Schema Registry is a service that provides a central repository for managing and validating schemas for data serialization formats such as Avro, JSON, and Protobuf. In the context of Apache Kafka, the Schema Registry is used to enforce schema evolution rules and ensure that data produced and consumed by Kafka topics adheres to a predefined schema.\n",
                "\n",
                "### Key Concepts\n",
                "\n",
                "1. **Schema**: A schema defines the structure of the data. It specifies the fields, their types, and any constraints on the data.\n",
                "\n",
                "2. **Schema Evolution**: Schema evolution refers to the process of updating schemas over time while maintaining compatibility with existing data. The Schema Registry enforces compatibility rules to ensure that changes to schemas do not break existing producers or consumers.\n",
                "\n",
                "3. **Compatibility Modes**: The Schema Registry supports different compatibility modes to control how schemas can evolve. Common compatibility modes include:\n",
                "    - **Backward Compatibility**: New schema versions can read data written by previous versions.\n",
                "    - **Forward Compatibility**: Previous schema versions can read data written by new versions.\n",
                "    - **Full Compatibility**: Both backward and forward compatibility are enforced.\n",
                "    - **Backward Transitive Compatibility**: New schema versions can read data written by all previous versions.\n",
                "    - **Forward Transitive Compatibility**: All previous schema versions can read data written by the new version.\n",
                "    - **Full Transitive Compatibility**: Both backward and forward transitive compatibility are enforced.\n",
                "\n",
                "4. **Schema Registry API**: The Schema Registry provides a RESTful API for managing schemas. Producers and consumers interact with the Schema Registry to retrieve and validate schemas.\n",
                "\n",
                "### Benefits of Using a Schema Registry\n",
                "\n",
                "1. **Data Quality**: Ensures that data produced and consumed by Kafka topics adheres to a predefined schema, improving data quality and consistency.\n",
                "\n",
                "2. **Schema Evolution**: Facilitates schema evolution by enforcing compatibility rules, allowing schemas to evolve without breaking existing applications.\n",
                "\n",
                "3. **Centralized Management**: Provides a central repository for managing schemas, making it easier to track and update schemas across multiple applications.\n",
                "\n",
                "4. **Interoperability**: Supports multiple serialization formats (Avro, JSON, Protobuf), enabling interoperability between different systems and applications.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Use Case\n",
                "\n",
                "Consider a Kafka topic that stores user events. By using a Schema Registry, you can define a schema for user events and ensure that all producers and consumers adhere to this schema. If you need to add a new field to the user event schema, the Schema Registry can enforce compatibility rules to ensure that existing consumers can still read the data.\n",
                "\n",
                "In summary, the Schema Registry is a powerful tool for managing and validating schemas in Apache Kafka, ensuring data quality, facilitating schema evolution, and providing centralized schema management."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Examples\n",
                "\n",
                "Below are examples of registering a JSON schema, a sample producer, and a sample consumer using the Schema Registry in Apache Kafka."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import requests\n",
                "import json\n",
                "\n",
                "# Schema Registry URL\n",
                "schema_registry_url = 'http://schema-registry:8081'\n",
                "\n",
                "# User JSON Schema\n",
                "user_schema = {\n",
                "    \"type\": \"object\",\n",
                "    \"properties\": {\n",
                "        \"id\": {\"type\": \"integer\"},\n",
                "        \"name\": {\"type\": \"string\"},\n",
                "        \"email\": {\"type\": \"string\"}\n",
                "    },\n",
                "    \"required\": [\"id\", \"name\", \"email\"]\n",
                "}\n",
                "\n",
                "# Register Schema\n",
                "response = requests.post(\n",
                "    f\"{schema_registry_url}/subjects/user-json-value/versions\",\n",
                "    headers={\"Content-Type\": \"application/vnd.schemaregistry.v1+json\"},\n",
                "    data=json.dumps({\"schema\": json.dumps(user_schema), \"schemaType\": \"JSON\"})\n",
                ")\n",
                "\n",
                "if response.status_code == 200:\n",
                "    print(\"Schema registered successfully!\")\n",
                "else:\n",
                "    print(f\"Failed to register schema: {response.text}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from confluent_kafka import Producer\n",
                "from confluent_kafka.serialization import StringSerializer, SerializationContext, MessageField\n",
                "from confluent_kafka.schema_registry import SchemaRegistryClient\n",
                "from confluent_kafka.schema_registry.json_schema import JSONSerializer\n",
                "import time\n",
                "import json\n",
                "\n",
                "# Kafka Configuration\n",
                "conf = {\n",
                "    'bootstrap.servers': \"kafka-broker-1:29094,kafka-broker-2:29094\"\n",
                "}\n",
                "\n",
                "# Schema Registry Configuration\n",
                "schema_registry_conf = {'url': 'http://schema-registry:8081'}\n",
                "schema_registry_client = SchemaRegistryClient(schema_registry_conf)\n",
                "\n",
                "# User Schema\n",
                "user_schema = {\n",
                "    \"type\": \"object\",\n",
                "    \"properties\": {\n",
                "        \"id\": {\"type\": \"integer\"},\n",
                "        \"name\": {\"type\": \"string\"},\n",
                "        \"email\": {\"type\": \"string\"}\n",
                "    },\n",
                "    \"required\": [\"id\", \"name\", \"email\"]\n",
                "}\n",
                "\n",
                "# Create JSON Serializer\n",
                "json_serializer = JSONSerializer(json.dumps(user_schema), schema_registry_client)\n",
                "\n",
                "# Create Producer Instance\n",
                "producer = Producer(conf)\n",
                "\n",
                "# Kafka Topic\n",
                "topic = \"user-json\"\n",
                "\n",
                "# Produce User Messages\n",
                "for i in range(10):\n",
                "    user = {'id': i, 'name': f\"User {i}\", 'email': f\"user{i}@example.com\"}\n",
                "    producer.produce(\n",
                "        topic=topic,\n",
                "        key=StringSerializer('utf_8')(str(i), SerializationContext(topic, MessageField.KEY)),\n",
                "        value=json_serializer(user, SerializationContext(topic, MessageField.VALUE))\n",
                "    )\n",
                "    print(f\"Produced: {user}\")\n",
                "    producer.flush()  # Ensure delivery\n",
                "    time.sleep(1)  # Simulate delay between messages\n",
                "\n",
                "print(\"All user messages produced successfully!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from confluent_kafka import Consumer, KafkaException\n",
                "from confluent_kafka.serialization import StringDeserializer, SerializationContext, MessageField\n",
                "from confluent_kafka.schema_registry import SchemaRegistryClient\n",
                "from confluent_kafka.schema_registry.json_schema import JSONDeserializer\n",
                "import json\n",
                "\n",
                "# Kafka Consumer Configuration\n",
                "conf = {\n",
                "    'bootstrap.servers': \"kafka-broker-1:29094,kafka-broker-2:29094\",\n",
                "    'group.id': 'user-group',\n",
                "    'auto.offset.reset': 'earliest'\n",
                "}\n",
                "\n",
                "# Schema Registry Configuration\n",
                "schema_registry_conf = {'url': 'http://schema-registry:8081'}\n",
                "schema_registry_client = SchemaRegistryClient(schema_registry_conf)\n",
                "\n",
                "# User Schema\n",
                "user_schema = {\n",
                "    \"type\": \"object\",\n",
                "    \"properties\": {\n",
                "        \"id\": {\"type\": \"integer\"},\n",
                "        \"name\": {\"type\": \"string\"},\n",
                "        \"email\": {\"type\": \"string\"}\n",
                "    },\n",
                "    \"required\": [\"id\", \"name\", \"email\"]\n",
                "}\n",
                "\n",
                "# Create JSON Deserializer\n",
                "def user_from_dict(data, ctx):\n",
                "    return data\n",
                "\n",
                "json_deserializer = JSONDeserializer(json.dumps(user_schema), from_dict=user_from_dict, schema_registry_client=schema_registry_client)\n",
                "\n",
                "# Create Consumer Instance\n",
                "consumer = Consumer(conf)\n",
                "topic = \"user-json\"\n",
                "consumer.subscribe([topic])\n",
                "\n",
                "# Consume User Messages\n",
                "try:\n",
                "    while True:\n",
                "        msg = consumer.poll(1.0)  # Poll for messages\n",
                "        if msg is None:\n",
                "            continue\n",
                "        if msg.error():\n",
                "            raise KafkaException(msg.error())\n",
                "\n",
                "        user = json_deserializer(msg.value(), SerializationContext(topic, MessageField.VALUE))\n",
                "        print(f\"Consumed: {user}\")\n",
                "\n",
                "except KeyboardInterrupt:\n",
                "    print(\"Stopping Consumer...\")\n",
                "finally:\n",
                "    consumer.close()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Registering an Avro Schema\n",
                "\n",
                "Below is an example of registering an Avro schema, a sample producer, and a sample consumer using the Schema Registry in Apache Kafka."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import requests\n",
                "import json\n",
                "\n",
                "# Schema Registry URL\n",
                "schema_registry_url = 'http://schema-registry:8081'\n",
                "\n",
                "# User Avro Schema\n",
                "user_avro_schema = {\n",
                "    \"type\": \"record\",\n",
                "    \"name\": \"User\",\n",
                "    \"fields\": [\n",
                "        {\"name\": \"id\", \"type\": \"int\"},\n",
                "        {\"name\": \"name\", \"type\": \"string\"},\n",
                "        {\"name\": \"email\", \"type\": \"string\"}\n",
                "    ]\n",
                "}\n",
                "\n",
                "# Register Avro Schema\n",
                "response = requests.post(\n",
                "    f\"{schema_registry_url}/subjects/user-avro-value/versions\",\n",
                "    headers={\"Content-Type\": \"application/vnd.schemaregistry.v1+json\"},\n",
                "    data=json.dumps({\"schema\": json.dumps(user_avro_schema)})\n",
                ")\n",
                "\n",
                "if response.status_code == 200:\n",
                "    print(\"Avro schema registered successfully!\")\n",
                "else:\n",
                "    print(f\"Failed to register Avro schema: {response.text}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from confluent_kafka import Producer\n",
                "from confluent_kafka.serialization import StringSerializer, SerializationContext, MessageField\n",
                "from confluent_kafka.schema_registry import SchemaRegistryClient, Schema\n",
                "from confluent_kafka.schema_registry.avro import AvroSerializer\n",
                "import time\n",
                "import avro.schema\n",
                "\n",
                "# Kafka Configuration\n",
                "conf = {\n",
                "    'bootstrap.servers': \"kafka-broker-1:29094,kafka-broker-2:29094\"\n",
                "}\n",
                "\n",
                "# Schema Registry Configuration\n",
                "schema_registry_conf = {'url': 'http://schema-registry:8081'}\n",
                "schema_registry_client = SchemaRegistryClient(schema_registry_conf)\n",
                "\n",
                "# User Avro Schema\n",
                "user_avro_schema_str = \"\"\"\n",
                "{\n",
                "    \"type\": \"record\",\n",
                "    \"name\": \"User\",\n",
                "    \"fields\": [\n",
                "        {\"name\": \"id\", \"type\": \"int\"},\n",
                "        {\"name\": \"name\", \"type\": \"string\"},\n",
                "        {\"name\": \"email\", \"type\": \"string\"}\n",
                "    ]\n",
                "}\n",
                "\"\"\"\n",
                "\n",
                "\n",
                "# Create Schema object\n",
                "user_schema = Schema(user_avro_schema_str, \"AVRO\")\n",
                "# Create Avro Serializer\n",
                "avro_serializer = AvroSerializer(schema_registry_client, user_schema)\n",
                "# Create Producer Instance\n",
                "producer = Producer(conf)\n",
                "# Kafka Topic\n",
                "topic = \"user-avro\"\n",
                "\n",
                "# Produce User Messages\n",
                "for i in range(10):\n",
                "    user = {'id': i, 'name': f\"User {i}\", 'email': f\"user{i}@example.com\"}\n",
                "    producer.produce(\n",
                "        topic=topic,\n",
                "        key=StringSerializer('utf_8')(str(i), SerializationContext(topic, MessageField.KEY)),\n",
                "        value=avro_serializer(user, SerializationContext(topic, MessageField.VALUE))\n",
                "    )\n",
                "    print(f\"Produced: {user}\")\n",
                "    producer.flush()  # Ensure delivery\n",
                "    time.sleep(1)  # Simulate delay between messages\n",
                "\n",
                "print(\"All user messages produced successfully!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from confluent_kafka import Consumer, KafkaException\n",
                "from confluent_kafka.serialization import StringDeserializer, SerializationContext, MessageField\n",
                "from confluent_kafka.schema_registry import SchemaRegistryClient\n",
                "from confluent_kafka.schema_registry.avro import AvroDeserializer\n",
                "\n",
                "# Kafka Consumer Configuration\n",
                "conf = {\n",
                "    'bootstrap.servers': \"kafka-broker-1:29094,kafka-broker-2:29094\",\n",
                "    'group.id': 'user-group',\n",
                "    'auto.offset.reset': 'earliest'\n",
                "}\n",
                "\n",
                "# Schema Registry Configuration\n",
                "schema_registry_conf = {'url': 'http://schema-registry:8081'}\n",
                "schema_registry_client = SchemaRegistryClient(schema_registry_conf)\n",
                "\n",
                "# User Avro Schema\n",
                "user_avro_schema_str = \"\"\"\n",
                "{\n",
                "    \"type\": \"record\",\n",
                "    \"name\": \"User\",\n",
                "    \"fields\": [\n",
                "        {\"name\": \"id\", \"type\": \"int\"},\n",
                "        {\"name\": \"name\", \"type\": \"string\"},\n",
                "        {\"name\": \"email\", \"type\": \"string\"}\n",
                "    ]\n",
                "}\n",
                "\"\"\"\n",
                "\n",
                "# Create Schema object\n",
                "user_schema = Schema(user_avro_schema_str, \"AVRO\")\n",
                "\n",
                "# Create Avro Deserializer\n",
                "def user_from_dict(data, ctx):\n",
                "    return data\n",
                "\n",
                "avro_deserializer = AvroDeserializer(schema_registry_client, user_schema, from_dict=user_from_dict)\n",
                "\n",
                "# Create Consumer Instance\n",
                "consumer = Consumer(conf)\n",
                "topic = \"user-avro\"\n",
                "consumer.subscribe([topic])\n",
                "\n",
                "# Consume User Messages\n",
                "try:\n",
                "    while True:\n",
                "        msg = consumer.poll(1.0)  # Poll for messages\n",
                "        if msg is None:\n",
                "            continue\n",
                "        if msg.error():\n",
                "            raise KafkaException(msg.error())\n",
                "\n",
                "        user = avro_deserializer(msg.value(), SerializationContext(topic, MessageField.VALUE))\n",
                "        print(f\"Consumed: {user}\")\n",
                "\n",
                "except KeyboardInterrupt:\n",
                "    print(\"Stopping Consumer...\")\n",
                "finally:\n",
                "    consumer.close()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Registering an Protobuf Schema\n",
                "\n",
                "Below is an example of registering an Protobuf schema, a sample producer, and a sample consumer using the Schema Registry in Apache Kafka."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import requests\n",
                "import json\n",
                "\n",
                "# Schema Registry URL\n",
                "schema_registry_url = 'http://schema-registry:8081'\n",
                "\n",
                "# User Protobuf Schema\n",
                "user_proto_schema = \"\"\"\n",
                "syntax = \"proto3\";\n",
                "package user;\n",
                "\n",
                "message User {\n",
                "    int32 id = 1;\n",
                "    string name = 2;\n",
                "    string email = 3;\n",
                "}\n",
                "\"\"\"\n",
                "\n",
                "# Register Protobuf Schema\n",
                "response = requests.post(\n",
                "    f\"{schema_registry_url}/subjects/user-proto-value/versions\",\n",
                "    headers={\"Content-Type\": \"application/vnd.schemaregistry.v1+json\"},\n",
                "    data=json.dumps({\"schema\": user_proto_schema, \"schemaType\": \"PROTOBUF\"})\n",
                ")\n",
                "\n",
                "if response.status_code == 200:\n",
                "    print(\"Protobuf schema registered successfully!\")\n",
                "else:\n",
                "    print(f\"Failed to register Protobuf schema: {response.text}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Produced: name: \"User 0\"\n",
                        "email: \"user0@example.com\"\n",
                        "\n",
                        "Produced: id: 1\n",
                        "name: \"User 1\"\n",
                        "email: \"user1@example.com\"\n",
                        "\n",
                        "Produced: id: 2\n",
                        "name: \"User 2\"\n",
                        "email: \"user2@example.com\"\n",
                        "\n",
                        "Produced: id: 3\n",
                        "name: \"User 3\"\n",
                        "email: \"user3@example.com\"\n",
                        "\n",
                        "Produced: id: 4\n",
                        "name: \"User 4\"\n",
                        "email: \"user4@example.com\"\n",
                        "\n",
                        "Produced: id: 5\n",
                        "name: \"User 5\"\n",
                        "email: \"user5@example.com\"\n",
                        "\n",
                        "Produced: id: 6\n",
                        "name: \"User 6\"\n",
                        "email: \"user6@example.com\"\n",
                        "\n",
                        "Produced: id: 7\n",
                        "name: \"User 7\"\n",
                        "email: \"user7@example.com\"\n",
                        "\n",
                        "Produced: id: 8\n",
                        "name: \"User 8\"\n",
                        "email: \"user8@example.com\"\n",
                        "\n",
                        "Produced: id: 9\n",
                        "name: \"User 9\"\n",
                        "email: \"user9@example.com\"\n",
                        "\n",
                        "All user messages produced successfully!\n"
                    ]
                }
            ],
            "source": [
                "from confluent_kafka import Producer\n",
                "from confluent_kafka.serialization import StringSerializer, SerializationContext, MessageField\n",
                "from confluent_kafka.schema_registry import SchemaRegistryClient\n",
                "from confluent_kafka.schema_registry.protobuf import ProtobufSerializer\n",
                "import time\n",
                "import sys\n",
                "\n",
                "# Add the src/proto directory to the Python path\n",
                "sys.path.append('/workspaces/kafka-tutorial/src/proto')\n",
                "\n",
                "import user_pb2\n",
                "\n",
                "# Kafka Configuration\n",
                "conf = {\n",
                "    'bootstrap.servers': \"kafka-broker-1:29094,kafka-broker-2:29094\"\n",
                "}\n",
                "\n",
                "# Schema Registry Configuration\n",
                "schema_registry_conf = {'url': 'http://schema-registry:8081'}\n",
                "schema_registry_client = SchemaRegistryClient(schema_registry_conf)\n",
                "\n",
                "# Protobuf Serializer Configuration\n",
                "protobuf_serializer_conf = {'use.deprecated.format': False}\n",
                "\n",
                "# Create Protobuf Serializer\n",
                "protobuf_serializer = ProtobufSerializer(user_pb2.User, schema_registry_client, protobuf_serializer_conf)\n",
                "\n",
                "# Create Producer Instance\n",
                "producer = Producer(conf)\n",
                "\n",
                "# Kafka Topic\n",
                "topic = \"user-protobuf\"\n",
                "\n",
                "# Produce User Messages\n",
                "for i in range(10):\n",
                "    user = user_pb2.User(id=i, name=f\"User {i}\", email=f\"user{i}@example.com\")\n",
                "    producer.produce(\n",
                "        topic=topic,\n",
                "        key=StringSerializer('utf_8')(str(i), SerializationContext(topic, MessageField.KEY)),\n",
                "        value=protobuf_serializer(user, SerializationContext(topic, MessageField.VALUE))\n",
                "    )\n",
                "    print(f\"Produced: {user}\")\n",
                "    producer.flush()  # Ensure delivery\n",
                "    time.sleep(1)  # Simulate delay between messages\n",
                "\n",
                "print(\"All user messages produced successfully!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "ename": "ModuleNotFoundError",
                    "evalue": "No module named 'user_pb2'",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mconfluent_kafka\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mschema_registry\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SchemaRegistryClient\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mconfluent_kafka\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mschema_registry\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprotobuf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ProtobufDeserializer\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01muser_pb2\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Kafka Configuration\u001b[39;00m\n\u001b[32m      8\u001b[39m conf = {\n\u001b[32m      9\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mbootstrap.servers\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mkafka-broker-1:29094,kafka-broker-2:29094\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     10\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mgroup.id\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser-protobuf-consumer-group\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     11\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mauto.offset.reset\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mearliest\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     12\u001b[39m }\n",
                        "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'user_pb2'"
                    ]
                }
            ],
            "source": [
                "from confluent_kafka import Consumer\n",
                "from confluent_kafka.serialization import StringDeserializer, SerializationContext, MessageField\n",
                "from confluent_kafka.schema_registry import SchemaRegistryClient\n",
                "from confluent_kafka.schema_registry.protobuf import ProtobufDeserializer\n",
                "import sys\n",
                "\n",
                "# Add the src/proto directory to the Python path\n",
                "sys.path.append('/workspaces/kafka-tutorial/src/proto')\n",
                "\n",
                "import user_pb2\n",
                "\n",
                "# Kafka Configuration\n",
                "conf = {\n",
                "    'bootstrap.servers': \"kafka-broker-1:29094,kafka-broker-2:29094\",\n",
                "    'group.id': \"user-protobuf-consumer-group\",\n",
                "    'auto.offset.reset': 'earliest'\n",
                "}\n",
                "\n",
                "# Schema Registry Configuration\n",
                "schema_registry_conf = {'url': 'http://schema-registry:8081'}\n",
                "schema_registry_client = SchemaRegistryClient(schema_registry_conf)\n",
                "\n",
                "# Protobuf Deserializer Configuration\n",
                "protobuf_deserializer_conf = {'use.deprecated.format': False}\n",
                "\n",
                "# Create Protobuf Deserializer\n",
                "protobuf_deserializer = ProtobufDeserializer(user_pb2.User, protobuf_deserializer_conf, schema_registry_client)\n",
                "\n",
                "# Create String Deserializer for the key\n",
                "string_deserializer = StringDeserializer('utf_8')\n",
                "\n",
                "# Create Consumer Instance\n",
                "consumer = Consumer(conf)\n",
                "\n",
                "# Kafka Topic\n",
                "topic = \"user-protobuf\"\n",
                "\n",
                "# Subscribe to the topic\n",
                "consumer.subscribe([topic])\n",
                "\n",
                "# Consume User Messages\n",
                "try:\n",
                "    while True:\n",
                "        msg = consumer.poll(1.0)  # Poll for messages with a timeout of 1 second\n",
                "        if msg is None:\n",
                "            continue\n",
                "        if msg.error():\n",
                "            print(f\"Consumer error: {msg.error()}\")\n",
                "            continue\n",
                "\n",
                "        user = protobuf_deserializer(msg.value(), SerializationContext(topic, MessageField.VALUE))\n",
                "        key = string_deserializer(msg.key(), SerializationContext(topic, MessageField.KEY))\n",
                "        print(f\"Consumed: key={key}, value={user}\")\n",
                "\n",
                "except KeyboardInterrupt:\n",
                "    pass\n",
                "finally:\n",
                "    # Close the consumer to commit final offsets and clean up resources\n",
                "    consumer.close()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
