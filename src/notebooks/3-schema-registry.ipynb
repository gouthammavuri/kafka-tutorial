{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Schema Registry in Apache Kafka\n",
                "\n",
                "A Schema Registry is a service that provides a central repository for managing and validating schemas for data serialization formats such as Avro, JSON, and Protobuf. In the context of Apache Kafka, the Schema Registry is used to enforce schema evolution rules and ensure that data produced and consumed by Kafka topics adheres to a predefined schema.\n",
                "\n",
                "### Key Concepts\n",
                "\n",
                "1. **Schema**: A schema defines the structure of the data. It specifies the fields, their types, and any constraints on the data.\n",
                "\n",
                "2. **Schema Evolution**: Schema evolution refers to the process of updating schemas over time while maintaining compatibility with existing data. The Schema Registry enforces compatibility rules to ensure that changes to schemas do not break existing producers or consumers.\n",
                "\n",
                "3. **Compatibility Modes**: The Schema Registry supports different compatibility modes to control how schemas can evolve. Common compatibility modes include:\n",
                "    - **Backward Compatibility**: New schema versions can read data written by previous versions.\n",
                "    - **Forward Compatibility**: Previous schema versions can read data written by new versions.\n",
                "    - **Full Compatibility**: Both backward and forward compatibility are enforced.\n",
                "    - **Backward Transitive Compatibility**: New schema versions can read data written by all previous versions.\n",
                "    - **Forward Transitive Compatibility**: All previous schema versions can read data written by the new version.\n",
                "    - **Full Transitive Compatibility**: Both backward and forward transitive compatibility are enforced.\n",
                "\n",
                "4. **Schema Registry API**: The Schema Registry provides a RESTful API for managing schemas. Producers and consumers interact with the Schema Registry to retrieve and validate schemas.\n",
                "\n",
                "### Benefits of Using a Schema Registry\n",
                "\n",
                "1. **Data Quality**: Ensures that data produced and consumed by Kafka topics adheres to a predefined schema, improving data quality and consistency.\n",
                "\n",
                "2. **Schema Evolution**: Facilitates schema evolution by enforcing compatibility rules, allowing schemas to evolve without breaking existing applications.\n",
                "\n",
                "3. **Centralized Management**: Provides a central repository for managing schemas, making it easier to track and update schemas across multiple applications.\n",
                "\n",
                "4. **Interoperability**: Supports multiple serialization formats (Avro, JSON, Protobuf), enabling interoperability between different systems and applications.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Use Case\n",
                "\n",
                "Consider a Kafka topic that stores user events. By using a Schema Registry, you can define a schema for user events and ensure that all producers and consumers adhere to this schema. If you need to add a new field to the user event schema, the Schema Registry can enforce compatibility rules to ensure that existing consumers can still read the data.\n",
                "\n",
                "In summary, the Schema Registry is a powerful tool for managing and validating schemas in Apache Kafka, ensuring data quality, facilitating schema evolution, and providing centralized schema management."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Examples\n",
                "\n",
                "Below are examples of registering a JSON schema, a sample producer, and a sample consumer using the Schema Registry in Apache Kafka."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import requests\n",
                "import json\n",
                "\n",
                "# Schema Registry URL\n",
                "schema_registry_url = 'http://schema-registry:8081'\n",
                "\n",
                "# User JSON Schema\n",
                "user_schema = {\n",
                "    \"type\": \"object\",\n",
                "    \"properties\": {\n",
                "        \"id\": {\"type\": \"integer\"},\n",
                "        \"name\": {\"type\": \"string\"},\n",
                "        \"email\": {\"type\": \"string\"}\n",
                "    },\n",
                "    \"required\": [\"id\", \"name\", \"email\"]\n",
                "}\n",
                "\n",
                "# Register Schema\n",
                "response = requests.post(\n",
                "    f\"{schema_registry_url}/subjects/user-json-value/versions\",\n",
                "    headers={\"Content-Type\": \"application/vnd.schemaregistry.v1+json\"},\n",
                "    data=json.dumps({\"schema\": json.dumps(user_schema), \"schemaType\": \"JSON\"})\n",
                ")\n",
                "\n",
                "if response.status_code == 200:\n",
                "    print(\"Schema registered successfully!\")\n",
                "else:\n",
                "    print(f\"Failed to register schema: {response.text}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from confluent_kafka import Producer\n",
                "from confluent_kafka.serialization import StringSerializer, SerializationContext, MessageField\n",
                "from confluent_kafka.schema_registry import SchemaRegistryClient\n",
                "from confluent_kafka.schema_registry.json_schema import JSONSerializer\n",
                "import time\n",
                "import json\n",
                "\n",
                "# Kafka Configuration\n",
                "conf = {\n",
                "    'bootstrap.servers': \"kafka-broker-1:29094,kafka-broker-2:29094\"\n",
                "}\n",
                "\n",
                "# Schema Registry Configuration\n",
                "schema_registry_conf = {'url': 'http://schema-registry:8081'}\n",
                "schema_registry_client = SchemaRegistryClient(schema_registry_conf)\n",
                "\n",
                "# User Schema\n",
                "user_schema = {\n",
                "    \"type\": \"object\",\n",
                "    \"properties\": {\n",
                "        \"id\": {\"type\": \"integer\"},\n",
                "        \"name\": {\"type\": \"string\"},\n",
                "        \"email\": {\"type\": \"string\"}\n",
                "    },\n",
                "    \"required\": [\"id\", \"name\", \"email\"]\n",
                "}\n",
                "\n",
                "# Create JSON Serializer\n",
                "json_serializer = JSONSerializer(json.dumps(user_schema), schema_registry_client)\n",
                "\n",
                "# Create Producer Instance\n",
                "producer = Producer(conf)\n",
                "\n",
                "# Kafka Topic\n",
                "topic = \"user-json-topic\"\n",
                "\n",
                "# Produce User Messages\n",
                "for i in range(10):\n",
                "    user = {'id': i, 'name': f\"User {i}\", 'email': f\"user{i}@example.com\"}\n",
                "    producer.produce(\n",
                "        topic=topic,\n",
                "        key=StringSerializer('utf_8')(str(i), SerializationContext(topic, MessageField.KEY)),\n",
                "        value=json_serializer(user, SerializationContext(topic, MessageField.VALUE))\n",
                "    )\n",
                "    print(f\"Produced: {user}\")\n",
                "    producer.flush()  # Ensure delivery\n",
                "    time.sleep(1)  # Simulate delay between messages\n",
                "\n",
                "print(\"All user messages produced successfully!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from confluent_kafka import Consumer, KafkaException\n",
                "from confluent_kafka.serialization import StringDeserializer, SerializationContext, MessageField\n",
                "from confluent_kafka.schema_registry import SchemaRegistryClient\n",
                "from confluent_kafka.schema_registry.json_schema import JSONDeserializer\n",
                "import json\n",
                "\n",
                "# Kafka Consumer Configuration\n",
                "conf = {\n",
                "    'bootstrap.servers': \"kafka-broker-1:29094,kafka-broker-2:29094\",\n",
                "    'group.id': 'user-group',\n",
                "    'auto.offset.reset': 'earliest'\n",
                "}\n",
                "\n",
                "# Schema Registry Configuration\n",
                "schema_registry_conf = {'url': 'http://schema-registry:8081'}\n",
                "schema_registry_client = SchemaRegistryClient(schema_registry_conf)\n",
                "\n",
                "# User Schema\n",
                "user_schema = {\n",
                "    \"type\": \"object\",\n",
                "    \"properties\": {\n",
                "        \"id\": {\"type\": \"integer\"},\n",
                "        \"name\": {\"type\": \"string\"},\n",
                "        \"email\": {\"type\": \"string\"}\n",
                "    },\n",
                "    \"required\": [\"id\", \"name\", \"email\"]\n",
                "}\n",
                "\n",
                "# Create JSON Deserializer\n",
                "def user_from_dict(data, ctx):\n",
                "    return data\n",
                "\n",
                "json_deserializer = JSONDeserializer(json.dumps(user_schema), from_dict=user_from_dict, schema_registry_client=schema_registry_client)\n",
                "\n",
                "# Create Consumer Instance\n",
                "consumer = Consumer(conf)\n",
                "topic = \"user-json-topic\"\n",
                "consumer.subscribe([topic])\n",
                "\n",
                "# Consume User Messages\n",
                "try:\n",
                "    while True:\n",
                "        msg = consumer.poll(1.0)  # Poll for messages\n",
                "        if msg is None:\n",
                "            continue\n",
                "        if msg.error():\n",
                "            raise KafkaException(msg.error())\n",
                "\n",
                "        user = json_deserializer(msg.value(), SerializationContext(topic, MessageField.VALUE))\n",
                "        print(f\"Consumed: {user}\")\n",
                "\n",
                "except KeyboardInterrupt:\n",
                "    print(\"Stopping Consumer...\")\n",
                "finally:\n",
                "    consumer.close()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
