{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db363dd9",
   "metadata": {},
   "source": [
    "# Kafka Offset Management\n",
    "\n",
    "## What are Offsets?\n",
    "\n",
    "In Apache Kafka, offsets are sequential IDs given to messages as they arrive in a partition. They serve as a unique identifier for a message within a partition and allow consumers to keep track of which messages they have already processed.\n",
    "\n",
    "Key characteristics of offsets:\n",
    "- Each partition has its own offset numbering starting from 0\n",
    "- Offsets are always increasing (though not necessarily by 1)\n",
    "- They act like a bookmark, allowing consumers to resume from where they left off\n",
    "- Offsets are managed per consumer group for each partition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c648efd3",
   "metadata": {},
   "source": [
    "## Offset Management in Consumer Groups\n",
    "\n",
    "Consumer groups in Kafka use offsets to track their progress through the partitions they're reading. Each consumer within a group is assigned specific partitions, and it manages the offset for those partitions.\n",
    "\n",
    "When a consumer reads a message, it needs to commit the offset to tell Kafka it has successfully processed that message. This is crucial for fault tolerance - if a consumer fails, another consumer in the group can take over and start reading from the last committed offset.\n",
    "\n",
    "### Where are Offsets Stored?\n",
    "\n",
    "- Prior to Kafka 0.9: Offsets were stored in Apache ZooKeeper\n",
    "- Kafka 0.9 and later: Offsets are stored in an internal Kafka topic called `__consumer_offsets`\n",
    "\n",
    "The `__consumer_offsets` topic contains messages with key-value pairs where:\n",
    "- Key: Consumer group ID, topic name, and partition number\n",
    "- Value: Offset value, metadata, timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b527ee2",
   "metadata": {},
   "source": [
    "## Offset Commit Strategies\n",
    "\n",
    "There are two main strategies for committing offsets:\n",
    "\n",
    "### 1. Automatic Commits\n",
    "\n",
    "Kafka can automatically commit offsets at a specified interval.\n",
    "\n",
    "**Pros:**\n",
    "- Simple to use - no explicit code needed\n",
    "- Regular commits with minimal code\n",
    "\n",
    "**Cons:**\n",
    "- Risk of duplicate processing if consumer fails between auto-commits\n",
    "- Less control over when offsets are committed\n",
    "\n",
    "### 2. Manual Commits\n",
    "\n",
    "Developers explicitly commit offsets in code.\n",
    "\n",
    "**Pros:**\n",
    "- Fine-grained control over when offsets are committed\n",
    "- Can commit after successfully processing messages\n",
    "\n",
    "**Cons:**\n",
    "- More complex code required\n",
    "- Developer must remember to handle commits properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8e0d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Auto-commit configuration in Python with Confluent Kafka\n",
    "from confluent_kafka import Consumer\n",
    "\n",
    "# Auto-commit example (commits every 5 seconds)\n",
    "consumer_config = {\n",
    "    'bootstrap.servers': 'localhost:9092',\n",
    "    'group.id': 'my-group',\n",
    "    'enable.auto.commit': True,\n",
    "    'auto.commit.interval.ms': 5000,\n",
    "    'default.topic.config': {'auto.offset.reset': 'earliest'}\n",
    "}\n",
    "\n",
    "consumer = Consumer(consumer_config)\n",
    "consumer.subscribe(['my-topic'])\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        msg = consumer.poll(1.0)\n",
    "        if msg is None:\n",
    "            continue\n",
    "        if msg.error():\n",
    "            print(f\"Consumer error: {msg.error()}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"Received: {msg.value().decode('utf-8')}\")\n",
    "        print(f\"Partition: {msg.partition()}, Offset: {msg.offset()}\")\n",
    "        # Processing happens here\n",
    "        # Offset is auto-committed every 5 seconds\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "finally:\n",
    "    consumer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4add13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Manual commit in Python with Confluent Kafka\n",
    "from confluent_kafka import Consumer\n",
    "\n",
    "# Manual commit example\n",
    "consumer_config = {\n",
    "    'bootstrap.servers': 'localhost:9092',\n",
    "    'group.id': 'my-group',\n",
    "    'enable.auto.commit': False,\n",
    "    'default.topic.config': {'auto.offset.reset': 'earliest'}\n",
    "}\n",
    "\n",
    "consumer = Consumer(consumer_config)\n",
    "consumer.subscribe(['my-topic'])\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        msg = consumer.poll(1.0)\n",
    "        if msg is None:\n",
    "            continue\n",
    "        if msg.error():\n",
    "            print(f\"Consumer error: {msg.error()}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"Received: {msg.value().decode('utf-8')}\")\n",
    "        print(f\"Partition: {msg.partition()}, Offset: {msg.offset()}\")\n",
    "        \n",
    "        # Process the message\n",
    "        try:\n",
    "            # Business logic here\n",
    "            # If processing is successful, commit the offset\n",
    "            consumer.commit(msg, asynchronous=False)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing message: {e}\")\n",
    "            # Handle the error - might decide not to commit\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "finally:\n",
    "    consumer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae26ac36",
   "metadata": {},
   "source": [
    "## Offset Reset Policy\n",
    "\n",
    "When a consumer starts up without a previously committed offset (or the committed offset no longer exists), it needs to know where to start reading. This is controlled by the `auto.offset.reset` configuration:\n",
    "\n",
    "- `earliest`: Start from the beginning of the partition\n",
    "- `latest`: Start from the end, only reading new messages\n",
    "- `none`: Throw an exception if no offset is found\n",
    "\n",
    "This setting is crucial as it determines what happens when:\n",
    "- A new consumer group is created\n",
    "- A consumer group's committed offsets have expired\n",
    "- A consumer subscribes to a new partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102fae09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Setting different offset reset policies with Confluent Kafka\n",
    "from confluent_kafka import Consumer\n",
    "\n",
    "# Start from earliest available message\n",
    "earliest_config = {\n",
    "    'bootstrap.servers': 'localhost:9092',\n",
    "    'group.id': 'earliest-group',\n",
    "    'auto.offset.reset': 'earliest'\n",
    "}\n",
    "\n",
    "earliest_consumer = Consumer(earliest_config)\n",
    "earliest_consumer.subscribe(['my-topic'])\n",
    "\n",
    "# Start from latest (only new messages)\n",
    "latest_config = {\n",
    "    'bootstrap.servers': 'localhost:9092',\n",
    "    'group.id': 'latest-group',\n",
    "    'auto.offset.reset': 'latest'\n",
    "}\n",
    "\n",
    "latest_consumer = Consumer(latest_config)\n",
    "latest_consumer.subscribe(['my-topic'])\n",
    "\n",
    "# Make sure to close the consumers when done\n",
    "# earliest_consumer.close()\n",
    "# latest_consumer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3e0e66",
   "metadata": {},
   "source": [
    "## Offset Management Challenges\n",
    "\n",
    "### 1. At-least-once vs. Exactly-once Semantics\n",
    "\n",
    "- **At-least-once**: Commit offsets after processing messages (common approach)\n",
    "  - Risk: If a consumer crashes after processing but before committing, the message will be reprocessed\n",
    "  \n",
    "- **Exactly-once**: Transactional processing (available in newer Kafka versions)\n",
    "  - Requires message processing and offset commits to be atomic\n",
    "  - Often requires idempotent consumers or transaction support\n",
    "\n",
    "### 2. Retention Period for Offsets\n",
    "\n",
    "Offset information has a configurable retention period (default: 7 days). If a consumer group is inactive for longer than this period, its offset information is lost.\n",
    "\n",
    "### 3. Rebalancing\n",
    "\n",
    "When consumers join or leave a group, Kafka rebalances partitions among the remaining consumers. Managing offsets correctly during rebalancing is crucial to avoid data loss or duplication."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626a4e5f",
   "metadata": {},
   "source": [
    "## Manual Offset Seeking\n",
    "\n",
    "Sometimes you need to explicitly seek to a specific offset in a partition. This is useful for:\n",
    "- Reprocessing data from a specific point in time\n",
    "- Skipping corrupted messages\n",
    "- Advanced error handling scenarios\n",
    "\n",
    "The Kafka client APIs provide methods to seek to specific offsets, beginnings, or ends of partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912273f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Manual seeking to specific offsets with Confluent Kafka\n",
    "from confluent_kafka import Consumer, TopicPartition\n",
    "\n",
    "config = {\n",
    "    'bootstrap.servers': 'localhost:9092',\n",
    "    'group.id': 'my-seek-group'\n",
    "}\n",
    "\n",
    "consumer = Consumer(config)\n",
    "\n",
    "# Create a TopicPartition object\n",
    "partition = TopicPartition('my-topic', 0, 1500)  # topic, partition, offset\n",
    "\n",
    "# Assign to specific partition\n",
    "consumer.assign([partition])\n",
    "\n",
    "# Seek to a specific offset (1500)\n",
    "consumer.seek(partition)\n",
    "\n",
    "# Alternatively, seek to the beginning of a partition\n",
    "# beginning_partition = TopicPartition('my-topic', 0)\n",
    "# consumer.seek_to_beginning(beginning_partition)\n",
    "\n",
    "# Or seek to the end of a partition\n",
    "# end_partition = TopicPartition('my-topic', 0)\n",
    "# consumer.seek_to_end(end_partition)\n",
    "\n",
    "# Start consuming from the specified offset\n",
    "try:\n",
    "    while True:\n",
    "        msg = consumer.poll(1.0)\n",
    "        if msg is None:\n",
    "            continue\n",
    "        if msg.error():\n",
    "            print(f\"Consumer error: {msg.error()}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"Received message at offset: {msg.offset()}\")\n",
    "        # Process message\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "finally:\n",
    "    consumer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a04dd86",
   "metadata": {},
   "source": [
    "## Best Practices for Offset Management\n",
    "\n",
    "1. **Idempotent Processing**: Design your consumers to handle the same message multiple times without negative effects\n",
    "\n",
    "2. **Commit Frequency**: Find the right balance - committing too frequently adds overhead, committing too rarely risks reprocessing more data after failures\n",
    "\n",
    "3. **Error Handling**: Decide whether to commit offsets for messages that couldn't be processed\n",
    "\n",
    "4. **Monitoring**: Track consumer lag (difference between latest offset and committed offset) to identify processing bottlenecks\n",
    "\n",
    "5. **Testing**: Test failure scenarios to ensure your offset management strategy works as expected\n",
    "\n",
    "6. **Transaction Support**: For critical applications, consider using Kafka's transactional capabilities (available in newer versions)\n",
    "\n",
    "7. **Batch Processing**: When processing in batches, only commit after the entire batch is successfully processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72498977",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Proper offset management is critical to building reliable Kafka-based applications. Understanding how offsets work and implementing the right commit strategy for your use case ensures:\n",
    "\n",
    "- Data is processed reliably even during failures\n",
    "- Consumers can resume from the right location after restarts\n",
    "- Message processing guarantees (at-least-once, exactly-once) are maintained\n",
    "\n",
    "The right offset management approach depends on your specific requirements around data processing guarantees, performance, and failure handling."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
