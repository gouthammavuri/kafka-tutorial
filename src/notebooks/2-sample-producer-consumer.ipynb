{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import Producer\n",
    "import time\n",
    "\n",
    "# Kafka Configuration\n",
    "conf = {\n",
    "    'bootstrap.servers': \"kafka-broker-1:29094,kafka-broker-2:29094\",\n",
    "}\n",
    "\n",
    "# Create Producer Instance\n",
    "producer = Producer(conf)\n",
    "\n",
    "# Kafka Topic\n",
    "topic = \"simple-messages-topic\"\n",
    "\n",
    "# Produce Messages\n",
    "for i in range(10):\n",
    "    message = f\"Message {i}\"\n",
    "    producer.produce(topic, key=None, value=message)\n",
    "    print(f\"Produced: {message}\")\n",
    "    producer.flush()  # Ensure delivery\n",
    "    time.sleep(1)  # Simulate delay between messages\n",
    "\n",
    "print(\"All messages produced successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import Consumer, KafkaException\n",
    "\n",
    "# Kafka Consumer Configuration\n",
    "conf = {\n",
    "    'bootstrap.servers': \"kafka-broker-1:29094,kafka-broker-2:29094\",\n",
    "    'group.id': 'sample-group',\n",
    "    'auto.offset.reset': 'earliest'  # Start consuming from the beginning\n",
    "}\n",
    "\n",
    "# Create Consumer Instance\n",
    "consumer = Consumer(conf)\n",
    "topic = \"simple-messages-topic\"\n",
    "consumer.subscribe([topic])\n",
    "\n",
    "# Consume Messages\n",
    "try:\n",
    "    while True:\n",
    "        msg = consumer.poll(1.0)  # Poll for messages\n",
    "        if msg is None:\n",
    "            continue\n",
    "        if msg.error():\n",
    "            raise KafkaException(msg.error())\n",
    "\n",
    "        print(f\"Consumed: {msg.value().decode('utf-8')}\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Stopping Consumer...\")\n",
    "finally:\n",
    "    consumer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kafka Serializers\n",
    "\n",
    "Serializers are used to convert objects into a format that can be sent over the network. Kafka supports different types of serializers:\n",
    "\n",
    "1. **StringSerializer**: Converts strings into bytes. This is useful when your messages are simple strings.\n",
    "2. **ByteArraySerializer**: Converts byte arrays into bytes. This is useful when your messages are already in byte array format.\n",
    "3. **AvroSerializer**: Converts Avro objects into bytes. This is useful when you are using Avro for schema management.\n",
    "4. **JsonSerializer**: Converts JSON objects into bytes. This is useful when your messages are JSON objects.\n",
    "5. **ProtobufSerializer**: Converts Protobuf objects into bytes. This is useful when you are using Protocol Buffers for schema management.\n",
    "\n",
    "Choosing the right serializer depends on the format of your data and your use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import Producer\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Kafka Configuration\n",
    "conf = {\n",
    "    'bootstrap.servers': \"kafka-broker-1:29094,kafka-broker-2:29094\"\n",
    "}\n",
    "\n",
    "# Create Producer Instance\n",
    "producer = Producer(conf)\n",
    "\n",
    "# Kafka Topic\n",
    "topic = \"json-messages-topic\"\n",
    "\n",
    "# Produce JSON Messages\n",
    "for i in range(10):\n",
    "    message = {'id': i, 'content': f\"Message {i}\"}\n",
    "    producer.produce(topic, key=None, value=json.dumps(message).encode('utf-8'))\n",
    "    print(f\"Produced: {message}\")\n",
    "    producer.flush()  # Ensure delivery\n",
    "    time.sleep(1)  # Simulate delay between messages\n",
    "\n",
    "print(\"All JSON messages produced successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import Consumer, KafkaException\n",
    "import json\n",
    "\n",
    "# Kafka Consumer Configuration\n",
    "conf = {\n",
    "    'bootstrap.servers': \"kafka-broker-1:29094,kafka-broker-2:29094\",\n",
    "    'group.id': 'sample-group', # Consumer Group ID\n",
    "    'auto.offset.reset': 'earliest'  # Start consuming from the beginning\n",
    "}\n",
    "\n",
    "# Create Consumer Instance\n",
    "consumer = Consumer(conf)\n",
    "topic = \"json-messages-topic\"\n",
    "consumer.subscribe([topic])\n",
    "\n",
    "# Consume JSON Messages\n",
    "try:\n",
    "    while True:\n",
    "        msg = consumer.poll(1.0)  # Poll for messages\n",
    "        if msg is None:\n",
    "            continue\n",
    "        if msg.error():\n",
    "            raise KafkaException(msg.error())\n",
    "\n",
    "        message = json.loads(msg.value().decode('utf-8'))\n",
    "        print(f\"Consumed: {message}\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Stopping Consumer...\")\n",
    "finally:\n",
    "    consumer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Event Types in the Same Topic Without Schema\n",
    "\n",
    "In Kafka, it is possible to publish multiple event types to the same topic. This can be useful in scenarios where different types of events are logically related and should be processed together. However, without schema references, managing and consuming these events can become challenging. Here are some considerations and strategies:\n",
    "\n",
    "1. **Event Type Identification**: Each event should include a field that identifies its type. This can be a simple string field like `event_type`.\n",
    "    ```json\n",
    "    {\n",
    "        \"event_type\": \"user_signup\",\n",
    "        \"user_id\": 123,\n",
    "        \"timestamp\": \"2023-10-01T12:34:56Z\"\n",
    "    }\n",
    "    ```\n",
    "\n",
    "2. **Event Validation**: Without schema references, it is important to validate the structure of each event type in the producer and consumer code. This can be done using custom validation logic or libraries like `jsonschema`.\n",
    "\n",
    "3. **Consumer Logic**: Consumers need to handle different event types appropriately. This can be achieved by checking the `event_type` field and processing the event accordingly.\n",
    "    ```python\n",
    "    if event['event_type'] == 'user_signup':\n",
    "        handle_user_signup(event)\n",
    "    elif event['event_type'] == 'order_placed':\n",
    "        handle_order_placed(event)\n",
    "    ```\n",
    "\n",
    "4. **Documentation**: Clearly document the structure of each event type and ensure that all producers and consumers adhere to these structures.\n",
    "\n",
    "5. **Testing**: Thoroughly test the producer and consumer logic to ensure that all event types are handled correctly.\n",
    "\n",
    "By following these strategies, you can effectively manage multiple event types in the same Kafka topic without relying on schema references."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges of Publishing Multiple Event Types in a Single Topic Without Schema\n",
    "\n",
    "While it's possible to publish different event types to the same topic, this approach presents several challenges when not using a schema registry:\n",
    "\n",
    "1. **Schema Evolution**: Without formal schema management, making changes to event structures becomes risky. Adding, removing, or modifying fields can break downstream consumers if they're not updated simultaneously.\n",
    "\n",
    "2. **Type Safety**: No built-in type checking means field types can be inconsistent or incorrect. A field intended to be a number might accidentally be sent as a string.\n",
    "\n",
    "3. **Versioning Complexity**: Managing different versions of events becomes manual and error-prone, requiring custom logic to handle backward/forward compatibility.\n",
    "\n",
    "4. **Increased Development Overhead**: Developers must implement validation logic in both producers and consumers, duplicating effort and potentially introducing inconsistencies.\n",
    "\n",
    "5. **Documentation Drift**: Without enforced schemas, documentation about event structures can become outdated or incomplete over time.\n",
    "\n",
    "6. **Runtime Errors**: Problems with event formats are typically discovered at runtime rather than compile/build time, leading to production issues.\n",
    "\n",
    "7. **Consumer Complexity**: Consumers need complex branching logic to handle different event types and their potential variations.\n",
    "\n",
    "8. **Performance Impact**: Without optimized serialization/deserialization that schemas provide, parsing and validation can be less efficient.\n",
    "\n",
    "9. **Governance Challenges**: Enforcing standards across teams becomes difficult without centralized schema validation.\n",
    "\n",
    "10. **Testing Burden**: Comprehensive testing becomes more critical and complex to ensure all event variations are properly handled.\n",
    "\n",
    "These challenges increase with system scale, number of event types, and frequency of changes. Using a schema registry like Confluent Schema Registry with Avro, Protobuf, or JSON Schema can address many of these issues by providing centralized schema management, validation, and evolution controls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import Producer\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Kafka Configuration\n",
    "conf = {\n",
    "    'bootstrap.servers': \"kafka-broker-1:29094,kafka-broker-2:29094\"\n",
    "}\n",
    "\n",
    "# Create Producer Instance\n",
    "producer = Producer(conf)\n",
    "\n",
    "# Kafka Topic\n",
    "topic = \"multi-event-types-topic\"\n",
    "\n",
    "# Produce Multiple Event Types\n",
    "events = [\n",
    "    {'event_type': 'user_signup', 'user_id': 1, 'timestamp': '2023-10-01T12:34:56Z'},\n",
    "    {'event_type': 'order_placed', 'order_id': 101, 'amount': 250.75, 'timestamp': '2023-10-01T12:35:56Z'},\n",
    "    {'event_type': 'user_signup', 'user_id': 2, 'timestamp': '2023-10-01T12:36:56Z'},\n",
    "    {'event_type': 'order_placed', 'order_id': 102, 'amount': 150.50, 'timestamp': '2023-10-01T12:37:56Z'}\n",
    "]\n",
    "\n",
    "for event in events:\n",
    "    producer.produce(topic, key=None, value=json.dumps(event).encode('utf-8'))\n",
    "    print(f\"Produced: {event}\")\n",
    "    producer.flush()  # Ensure delivery\n",
    "    time.sleep(1)  # Simulate delay between messages\n",
    "\n",
    "print(\"All events produced successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import Consumer, KafkaException\n",
    "import json\n",
    "\n",
    "# Kafka Consumer Configuration\n",
    "conf = {\n",
    "    'bootstrap.servers': \"kafka-broker-1:29094,kafka-broker-2:29094\",\n",
    "    'group.id': 'multi-event-group',\n",
    "    'auto.offset.reset': 'earliest'  # Start consuming from the beginning\n",
    "}\n",
    "\n",
    "# Create Consumer Instance\n",
    "consumer = Consumer(conf)\n",
    "topic = \"multi-event-types-topic\"\n",
    "consumer.subscribe([topic])\n",
    "\n",
    "# Consume Multiple Event Types\n",
    "try:\n",
    "    while True:\n",
    "        msg = consumer.poll(1.0)  # Poll for messages\n",
    "        if msg is None:\n",
    "            continue\n",
    "        if msg.error():\n",
    "            raise KafkaException(msg.error())\n",
    "\n",
    "        event = json.loads(msg.value().decode('utf-8'))\n",
    "        event_type = event.get('event_type')\n",
    "\n",
    "        if event_type == 'user_signup':\n",
    "            print(f\"User Signup Event: {event}\")\n",
    "        elif event_type == 'order_placed':\n",
    "            print(f\"Order Placed Event: {event}\")\n",
    "        else:\n",
    "            print(f\"Unknown Event Type: {event}\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Stopping Consumer...\")\n",
    "finally:\n",
    "    consumer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
