{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema Evolution: Forward Compatibility\n",
    "\n",
    "Forward compatibility ensures that consumers using the new schema can read data produced with the old schema. This is useful when you want to add new fields to your schema without breaking existing consumers.\n",
    "\n",
    "### Example: Adding a New Field to the User Schema\n",
    "\n",
    "Let's evolve the existing User Avro schema by adding a new field `age`. When adding a new field to a schema for forward compatibility, it is important to provide a default value. This ensures that the new schema can read data produced with the old schema, which does not contain the new field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Schema Registry URL\n",
    "schema_registry_url = 'http://schema-registry:8081'\n",
    "\n",
    "# Evolved User Avro Schema\n",
    "evolved_user_avro_schema = {\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"User\",\n",
    "    \"fields\": [\n",
    "        {\"name\": \"id\", \"type\": \"int\"},\n",
    "        {\"name\": \"name\", \"type\": \"string\"},\n",
    "        {\"name\": \"email\", \"type\": \"string\"},\n",
    "        {\"name\": \"age\", \"type\": \"int\", \"default\": 0}  # New field with default value\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Register Evolved Avro Schema\n",
    "response = requests.post(\n",
    "    f\"{schema_registry_url}/subjects/user-avro-value/versions\",\n",
    "    headers={\"Content-Type\": \"application/vnd.schemaregistry.v1+json\"},\n",
    "    data=json.dumps({\"schema\": json.dumps(evolved_user_avro_schema)})\n",
    ")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(\"Evolved Avro schema registered successfully!\")\n",
    "else:\n",
    "    print(f\"Failed to register evolved Avro schema: {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import Producer\n",
    "from confluent_kafka.serialization import StringSerializer, SerializationContext, MessageField\n",
    "from confluent_kafka.schema_registry import SchemaRegistryClient, Schema\n",
    "from confluent_kafka.schema_registry.avro import AvroSerializer\n",
    "import time\n",
    "import avro.schema\n",
    "\n",
    "# Kafka Configuration\n",
    "conf = {\n",
    "    'bootstrap.servers': \"kafka-broker-1:29094,kafka-broker-2:29094\"\n",
    "}\n",
    "\n",
    "# Schema Registry Configuration\n",
    "schema_registry_conf = {'url': 'http://schema-registry:8081'}\n",
    "schema_registry_client = SchemaRegistryClient(schema_registry_conf)\n",
    "\n",
    "# Evolved User Avro Schema\n",
    "evolved_user_avro_schema_str = \"\"\"\n",
    "{\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"User\",\n",
    "    \"fields\": [\n",
    "        {\"name\": \"id\", \"type\": \"int\"},\n",
    "        {\"name\": \"name\", \"type\": \"string\"},\n",
    "        {\"name\": \"email\", \"type\": \"string\"},\n",
    "        {\"name\": \"age\", \"type\": \"int\", \"default\": 0}\n",
    "    ]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Create Schema object\n",
    "evolved_user_schema = Schema(evolved_user_avro_schema_str, \"AVRO\")\n",
    "# Create Avro Serializer\n",
    "avro_serializer = AvroSerializer(schema_registry_client, evolved_user_schema)\n",
    "# Create Producer Instance\n",
    "producer = Producer(conf)\n",
    "# Kafka Topic\n",
    "topic = \"user-avro\"\n",
    "\n",
    "# Produce User Messages with Evolved Schema\n",
    "for i in range(10):\n",
    "    user = {'id': i, 'name': f\"User {i}\", 'email': f\"user{i}@example.com\", 'age': 25 + i}\n",
    "    producer.produce(\n",
    "        topic=topic,\n",
    "        key=StringSerializer('utf_8')(str(i), SerializationContext(topic, MessageField.KEY)),\n",
    "        value=avro_serializer(user, SerializationContext(topic, MessageField.VALUE))\n",
    "    )\n",
    "    print(f\"Produced: {user}\")\n",
    "    producer.flush()  # Ensure delivery\n",
    "    time.sleep(1)  # Simulate delay between messages\n",
    "\n",
    "print(\"All user messages with evolved schema produced successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import Consumer\n",
    "from confluent_kafka.serialization import StringDeserializer, SerializationContext, MessageField\n",
    "from confluent_kafka.schema_registry.avro import AvroDeserializer\n",
    "\n",
    "# Consumer Configuration\n",
    "consumer_conf = {\n",
    "    'bootstrap.servers': \"kafka-broker-1:29094,kafka-broker-2:29094\",\n",
    "    'group.id': 'user-avro-consumer-group',\n",
    "    'auto.offset.reset': 'earliest'\n",
    "}\n",
    "\n",
    "# Create Consumer Instance\n",
    "consumer = Consumer(consumer_conf)\n",
    "# Subscribe to the topic\n",
    "consumer.subscribe([\"user-avro\"])\n",
    "\n",
    "# Old User Avro Schema (without 'age' field)\n",
    "old_user_avro_schema_str = \"\"\"\n",
    "{\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"User\",\n",
    "    \"fields\": [\n",
    "        {\"name\": \"id\", \"type\": \"int\"},\n",
    "        {\"name\": \"name\", \"type\": \"string\"},\n",
    "        {\"name\": \"email\", \"type\": \"string\"}\n",
    "    ]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Create Avro Deserializer\n",
    "old_user_schema = Schema(old_user_avro_schema_str, \"AVRO\")\n",
    "avro_deserializer = AvroDeserializer(schema_registry_client, old_user_schema)\n",
    "\n",
    "# Consume Messages\n",
    "try:\n",
    "    while True:\n",
    "        msg = consumer.poll(1.0)\n",
    "        if msg is None:\n",
    "            continue\n",
    "        if msg.error():\n",
    "            print(f\"Consumer error: {msg.error()}\")\n",
    "            continue\n",
    "        user = avro_deserializer(msg.value(), SerializationContext(msg.topic(), MessageField.VALUE))\n",
    "        print(f\"Consumed: {user}\")\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "finally:\n",
    "    consumer.close()\n",
    "\n",
    "print(\"All user messages with old schema consumed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema Evolution: Backward Compatibility\n",
    "\n",
    "Backward compatibility ensures that consumers using the old schema can read data produced with the new schema. This is critical when upgrading producers before consumers.\n",
    "\n",
    "### Example: Removing a Field from the User Schema\n",
    "\n",
    "Let's evolve the User Avro schema by removing the `age` field. For backward compatibility, we need to ensure that consumers using the older schema (which includes the `age` field) can still process messages produced with the new schema (which doesn't include the `age` field)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Schema Registry URL\n",
    "schema_registry_url = 'http://schema-registry:8081'\n",
    "\n",
    "# Backward Compatible User Avro Schema (removing the 'age' field)\n",
    "backward_user_avro_schema = {\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"User\",\n",
    "    \"fields\": [\n",
    "        {\"name\": \"id\", \"type\": \"int\"},\n",
    "        {\"name\": \"name\", \"type\": \"string\"},\n",
    "        {\"name\": \"email\", \"type\": \"string\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Register Backward Compatible Avro Schema to the SAME topic subject\n",
    "response = requests.post(\n",
    "    f\"{schema_registry_url}/subjects/user-avro-value/versions\",  # Note: Using the same subject as before\n",
    "    headers={\"Content-Type\": \"application/vnd.schemaregistry.v1+json\"},\n",
    "    data=json.dumps({\"schema\": json.dumps(backward_user_avro_schema)})\n",
    ")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(\"Backward compatible Avro schema registered successfully to the same topic!\")\n",
    "else:\n",
    "    print(f\"Failed to register backward compatible Avro schema: {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import Producer\n",
    "from confluent_kafka.serialization import StringSerializer, SerializationContext, MessageField\n",
    "from confluent_kafka.schema_registry import SchemaRegistryClient, Schema\n",
    "from confluent_kafka.schema_registry.avro import AvroSerializer\n",
    "import time\n",
    "\n",
    "# Kafka Configuration\n",
    "conf = {\n",
    "    'bootstrap.servers': \"kafka-broker-1:29094,kafka-broker-2:29094\"\n",
    "}\n",
    "\n",
    "# Schema Registry Configuration\n",
    "schema_registry_conf = {'url': 'http://schema-registry:8081'}\n",
    "schema_registry_client = SchemaRegistryClient(schema_registry_conf)\n",
    "\n",
    "# Backward Compatible User Avro Schema\n",
    "backward_user_avro_schema_str = \"\"\"\n",
    "{\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"User\",\n",
    "    \"fields\": [\n",
    "        {\"name\": \"id\", \"type\": \"int\"},\n",
    "        {\"name\": \"name\", \"type\": \"string\"},\n",
    "        {\"name\": \"email\", \"type\": \"string\"}\n",
    "    ]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Create Schema object\n",
    "backward_user_schema = Schema(backward_user_avro_schema_str, \"AVRO\")\n",
    "# Create Avro Serializer\n",
    "avro_serializer = AvroSerializer(schema_registry_client, backward_user_schema)\n",
    "# Create Producer Instance\n",
    "producer = Producer(conf)\n",
    "# Kafka Topic - Use the SAME topic as before\n",
    "topic = \"user-avro\"\n",
    "\n",
    "# Produce User Messages with Backward Compatible Schema\n",
    "for i in range(10):\n",
    "    user = {'id': i + 100, 'name': f\"User {i + 100}\", 'email': f\"user{i + 100}@example.com\"}\n",
    "    producer.produce(\n",
    "        topic=topic,\n",
    "        key=StringSerializer('utf_8')(str(i + 100), SerializationContext(topic, MessageField.KEY)),\n",
    "        value=avro_serializer(user, SerializationContext(topic, MessageField.VALUE))\n",
    "    )\n",
    "    print(f\"Produced (new schema without age): {user}\")\n",
    "    producer.flush()  # Ensure delivery\n",
    "    time.sleep(1)  # Simulate delay between messages\n",
    "\n",
    "print(\"All user messages with backward compatible schema produced successfully to the same topic!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import Consumer\n",
    "from confluent_kafka.serialization import SerializationContext, MessageField\n",
    "from confluent_kafka.schema_registry.avro import AvroDeserializer\n",
    "\n",
    "# Consumer Configuration\n",
    "consumer_conf = {\n",
    "    'bootstrap.servers': \"kafka-broker-1:29094,kafka-broker-2:29094\",\n",
    "    'group.id': 'user-same-topic-consumer-group',\n",
    "    'auto.offset.reset': 'earliest'\n",
    "}\n",
    "\n",
    "# Create Consumer Instance\n",
    "consumer = Consumer(consumer_conf)\n",
    "# Subscribe to the same topic as before\n",
    "consumer.subscribe([\"user-avro\"])\n",
    "\n",
    "# Old User Avro Schema (with 'age' field)\n",
    "old_user_with_age_schema_str = \"\"\"\n",
    "{\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"User\",\n",
    "    \"fields\": [\n",
    "        {\"name\": \"id\", \"type\": \"int\"},\n",
    "        {\"name\": \"name\", \"type\": \"string\"},\n",
    "        {\"name\": \"email\", \"type\": \"string\"},\n",
    "        {\"name\": \"age\", \"type\": \"int\", \"default\": 0}\n",
    "    ]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Create Avro Deserializer with old schema (including age)\n",
    "old_schema_with_age = Schema(old_user_with_age_schema_str, \"AVRO\")\n",
    "avro_deserializer = AvroDeserializer(schema_registry_client, old_schema_with_age)\n",
    "\n",
    "# Consume Messages\n",
    "try:\n",
    "    count = 0\n",
    "    while count < 15:  # Increased to catch both old and new messages\n",
    "        msg = consumer.poll(1.0)\n",
    "        if msg is None:\n",
    "            continue\n",
    "        if msg.error():\n",
    "            print(f\"Consumer error: {msg.error()}\")\n",
    "            continue\n",
    "        user = avro_deserializer(msg.value(), SerializationContext(msg.topic(), MessageField.VALUE))\n",
    "        if user['id'] < 100:\n",
    "            print(f\"Consumed original message with age field: {user}\")\n",
    "        else:\n",
    "            print(f\"Consumed new message with defaulted age (0): {user}\")\n",
    "        count += 1\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "finally:\n",
    "    consumer.close()\n",
    "\n",
    "print(\"Successfully consumed messages with both schema versions from the same topic!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema Compatibility Summary\n",
    "\n",
    "- **Forward Compatibility**: New schema can read old data (Adding fields with defaults)\n",
    "- **Backward Compatibility**: Old schema can read new data (Removing fields or adding optional fields)\n",
    "- **Full Compatibility**: Schemas are both forward and backward compatible\n",
    "\n",
    "Schema Registry enforces these compatibility rules, preventing incompatible schema changes from breaking your data pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Considerations for Schema Compatibility\n",
    "\n",
    "### Default Values are Critical\n",
    "\n",
    "1. **Default Values for Forward Compatibility**:\n",
    "   - When adding new fields, they **must** have default values specified\n",
    "   - Without defaults, consumers using the new schema will fail when reading old data (where the new field doesn't exist)\n",
    "   - Example: `{\"name\": \"age\", \"type\": \"int\", \"default\": 0}`\n",
    "\n",
    "2. **Default Values for Backward Compatibility**:\n",
    "   - Old consumers expecting fields that no longer exist in the new schema need those fields to have defaults\n",
    "   - The default value is what old consumers will receive when the field is absent in the new data\n",
    "\n",
    "3. **Types and Default Value Consistency**:\n",
    "   - Default values must match the field type\n",
    "   - For complex types, ensure default values are valid according to the schema\n",
    "\n",
    "4. **Breaking Changes to Avoid**:\n",
    "   - Changing field types (e.g., from `int` to `string`)\n",
    "   - Removing fields without previously having defaults\n",
    "   - Changing the schema name or namespace\n",
    "   - Adding required fields without defaults\n",
    "\n",
    "5. **Schema Evolution Best Practices**:\n",
    "   - Start with a minimal schema and evolve gradually\n",
    "   - Always test compatibility before deploying schema changes to production\n",
    "   - Use the Schema Registry's compatibility checking features\n",
    "   - Document all schema changes and their compatibility implications\n",
    "\n",
    "Remember: Without proper default values, schema evolution will fail, and your producers or consumers will encounter errors when processing messages."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
